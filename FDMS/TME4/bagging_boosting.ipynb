{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fouille de données et medias sociaux\n",
    "# TP4 : Bagging et Boosting\n",
    "\n",
    "## Exercice 1 Bagging\n",
    "### Question 1\n",
    "Écrivez une fonction (ou une classe) implémentant la méthode Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: Parallelization\n",
    "# TODO: Bootstrap on features\n",
    "\n",
    "class BaggingClassifier(ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, n_estimators=5, \n",
    "                 max_samples=1.0, bootstrap=True, verbose=0,\n",
    "                oob_score=False):\n",
    "        if base_estimator is None:\n",
    "            self.base_estimator = DecisionTreeClassifier\n",
    "        else:\n",
    "            self.base_estimator = base_estimator\n",
    "            \n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.bootstrap = bootstrap\n",
    "        self.verbose = verbose\n",
    "        self.oob_score = oob_score\n",
    "        \n",
    "        \n",
    "        self.estimators = None\n",
    "        self.oob_score_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimators = []\n",
    "        if self.oob_score:\n",
    "            self.oob_counters = [Counter() for elt in X]\n",
    "            # self.oob_counters[i] = predictions for sample i\n",
    "        if self.verbose >= 1:\n",
    "            print(\"##### Fitting the classifiers #####\")\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.verbose >= 1:\n",
    "                print(\"Fitting classifier n°%d...\" % i)\n",
    "            clf = self.base_estimator()\n",
    "            sample_idx = np.random.choice(X.shape[0], \n",
    "                                          int(X.shape[0]*self.max_samples), \n",
    "                                          replace=self.bootstrap)\n",
    "            if self.verbose >= 2:\n",
    "                print(\"Random indexes:\", sample_idx)\n",
    "            clf.fit(X[sample_idx], y[sample_idx])\n",
    "            if self.verbose >= 1:\n",
    "                print(\"Done.\")\n",
    "            \n",
    "            if self.oob_score:\n",
    "                oob_samples = np.setdiff1d(np.arange(X.shape[0]), sample_idx)\n",
    "                preds = clf.predict(X)\n",
    "                for i in oob_samples:\n",
    "                    self.oob_counters[i].update([preds[i]])\n",
    "                    \n",
    "            self.estimators.append(clf)\n",
    "          \n",
    "        if self.oob_score:\n",
    "            if Counter() in self.oob_counters:\n",
    "                print(\"Warning, some samples don't have OOB estimations, \\\n",
    "                        the number of estimators seems to be too low.\")\n",
    "            else:\n",
    "                self.oob_predictions = [c.most_common(1)[0][0] for c in self.oob_counters]\n",
    "                errors = np.count_nonzero(self.oob_predictions != y)\n",
    "                self.oob_score_ = errors / X.shape[0]\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(\"##### Fitting is over #####\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.empty((X.shape[0], len(self.estimators)))\n",
    "        # predictions[i][j] = self.estimators[j].predict(X[i])\n",
    "        # TODO: More efficient method: only store new class and counters\n",
    "        for j, clf in enumerate(self.estimators):\n",
    "            predictions[:, j] = clf.predict(X)\n",
    "        \n",
    "        most_common = [Counter(line).most_common(1)[0][0] for line in predictions]\n",
    "        return most_common\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(self.predict(X), y)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2\n",
    "Appliquez cette méthode sur des arbres de décisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n",
      "Classes in y: (array([0, 1, 2]), array([50, 50, 50]))\n",
      "Scores with bag_clf: 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Classes in y:\", np.unique(y, return_counts=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier, oob_score=True,\n",
    "                            n_estimators=100, verbose=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"Scores with bag_clf:\", bag_clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3\n",
    "Évaluez et commentez l’erreur en généralisation par rapport à un arbre unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the BaggingClassifier, OOB error: 0.06666666666666667\n",
      "For the DecisionTreeClassifier, OOB error: 0.0888888888889\n",
      "Best parameters found for the DecisionTreeClassifier: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 0.25074999999999997, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "print(\"For the BaggingClassifier, OOB error:\", bag_clf.oob_score_)\n",
    "\n",
    "# Evaluating the OOB error on a decision tree:\n",
    "\n",
    "# Parameters for the Grid Search:\n",
    "params = {\"criterion\":(\"gini\", \"entropy\"),\n",
    "          \"splitter\":(\"best\", \"random\"),\n",
    "          \"max_depth\":np.arange(5, 51, 5),\n",
    "          \"min_samples_split\":np.linspace(1e-3, 1., 5)\n",
    "            }\n",
    "\n",
    "gridsearch = GridSearchCV(DecisionTreeClassifier(), params)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(\"For the DecisionTreeClassifier, OOB error:\", 1 - gridsearch.score(X_test, y_test))\n",
    "print(\"Best parameters found for the DecisionTreeClassifier:\", gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercice 2 Boosting\n",
    "### Question 1\n",
    "**Montrer que le coefficient de pondération des hypothèses vaut bien ce qu’il vaut.**\n",
    "\n",
    "Soit un problème d'apprentissage avec $((x_i, y_i)_{i \\in \\{1..n\\}}$, $x_i \\in X$, $y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "Après l'itération n° $m-1$, notre classifieur Boost est une combinaison linéaire de classifieurs faibles $h_i:X \\rightarrow \\{-1, +1\\}$ pondéré par les coefficients $a_i$:\n",
    "\n",
    "$C_{m-1}(x) = \\sum_{i=1}^{m-1}{a_i ⋅ h_i(x)}$.\n",
    "\n",
    "On veut trouver un nouvel estimateur faible $h_m$ pour avoir l'estimateur Boost à l'itération $m$.\n",
    "\n",
    "Soit l'erreur totale de $C_m$ : $E = \\sum_{i=1}^n{e^{-y_i ⋅ C_m(x_i)}}$, \n",
    "\n",
    "posons $w_i^1 = 1$ et $w_i^m = e^{-y_i ⋅ C_{m-1}(x_i)}$ pour $1 < m$, \n",
    "on a alors :\n",
    "\n",
    "$E = \\sum_{i=1}^n{e^{-y_i ⋅ C_m(x_i)}} \\\\\n",
    "E = \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}} + \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} \\\\\n",
    "E = \\sum_{i=1}^n{w_i^m ⋅ e^{-a_m}} + \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅(e^{a_m} - e^{-a_m})} $\n",
    "\n",
    "Ainsi l'estimateur faible $h_m$ qui minimise $E$ est celui qui minimise $\\sum_{y_i \\neq h_m(x_i)}{w_i^m}$, c'est-à-dire le classifieur qui fait le moins d'erreurs de classification, avec la pondération $w_i^m$.\n",
    "\n",
    "Après avoir trouvé ce classifieur $h_m$, il faut trouver le coefficient $a_m$, c'est le coefficient qui minimise $E$ :\n",
    "\n",
    "$\\frac {dE}{da_m} = \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} - \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}}$\n",
    "\n",
    "$\\frac {dE}{da_m} = 0 \\Leftrightarrow \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} - \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}} = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}}  = \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}}\\\\\n",
    "\\Leftrightarrow e^{2⋅a_m}⋅\\sum_{y_i \\neq h_m(x_i)}{w_i^m}  = \\sum_{y_i = h_m(x_i)}{w_i^m}\\\\\n",
    "\\Leftrightarrow 2⋅a_m = ln(\\frac {\\sum_{y_i = h_m(x_i)}{w_i^m})}{\\sum_{y_i \\neq h_m(x_i)}{w_i^m} })\\\\\n",
    "\\Leftrightarrow a_m = \\frac 1 2 ln(\\frac {\\sum_{y_i = h_m(x_i)}{w_i^m})}{\\sum_{y_i \\neq h_m(x_i)}{w_i^m} })$\n",
    "\n",
    "\n",
    "### Question 2\n",
    "Écrivez une fonction (ou une classe) implémentant la méthode AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Comment adapter un classifieur qui ne gère pas naturellement des pondérations pour qu’il puisse en tenir\n",
    "compte ?\n",
    "### Question 4\n",
    "Appliquez cette méthode sur des stumps (arbres de décision à un nœud).\n",
    "### Question 5\n",
    "Tracez les courbes de l’erreur d’apprentissage et de l’erreur de test en fonction du nombre de classifieurs.\n",
    "1\n",
    "### Question 6\n",
    "Affichez les points associés aux poids les plus élevés (donc les plus difficiles à classer). Sont-ils bien classés\n",
    "désormais ? Étaient-ils bien classé avec un seul arbre de décision ? Avec le Bagging ?\n",
    "### Question 7\n",
    "Appliquez AdaBoost à des arbres de décision de profondeur plus grande.\n",
    "Comment se comporte la généralisation quand la profondeur des arbres augmente ?\n",
    "2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
