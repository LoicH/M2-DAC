{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fouille de données et medias sociaux\n",
    "# TP4 : Bagging et Boosting\n",
    "\n",
    "## Exercice 1 Bagging\n",
    "### Question 1\n",
    "Écrivez une fonction (ou une classe) implémentant la méthode Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# TODO: Parallelization\n",
    "# TODO: Bootstrap on features\n",
    "\n",
    "class BaggingClassifier(ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, n_estimators=5, \n",
    "                 max_samples=1.0, bootstrap=True, verbose=0,\n",
    "                oob_score=False):\n",
    "        if base_estimator is None:\n",
    "            self.base_estimator = DecisionTreeClassifier\n",
    "        else:\n",
    "            self.base_estimator = base_estimator\n",
    "            \n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.bootstrap = bootstrap\n",
    "        self.verbose = verbose\n",
    "        self.oob_score = oob_score\n",
    "        \n",
    "        \n",
    "        self.estimators = None\n",
    "        self.oob_score_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimators = []\n",
    "        if self.oob_score:\n",
    "            self.oob_counters = [Counter() for elt in X]\n",
    "            # self.oob_counters[i] = predictions for sample i\n",
    "        if self.verbose >= 1:\n",
    "            print(\"##### Fitting the classifiers #####\")\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.verbose >= 1:\n",
    "                print(\"Fitting classifier n°%d...\" % i)\n",
    "            clf = self.base_estimator()\n",
    "            sample_idx = np.random.choice(X.shape[0], \n",
    "                                          int(X.shape[0]*self.max_samples), \n",
    "                                          replace=self.bootstrap)\n",
    "            if self.verbose >= 2:\n",
    "                print(\"Random indexes:\", sample_idx)\n",
    "            clf.fit(X[sample_idx], y[sample_idx])\n",
    "            if self.verbose >= 1:\n",
    "                print(\"Done.\")\n",
    "            \n",
    "            if self.oob_score:\n",
    "                oob_samples = np.setdiff1d(np.arange(X.shape[0]), sample_idx)\n",
    "                preds = clf.predict(X)\n",
    "                for i in oob_samples:\n",
    "                    self.oob_counters[i].update([preds[i]])\n",
    "                    \n",
    "            self.estimators.append(clf)\n",
    "          \n",
    "        if self.oob_score:\n",
    "            if Counter() in self.oob_counters:\n",
    "                print(\"Warning, some samples don't have OOB estimations, \\\n",
    "                        the number of estimators seems to be too low.\")\n",
    "            else:\n",
    "                self.oob_predictions = [c.most_common(1)[0][0] for c in self.oob_counters]\n",
    "                errors = np.count_nonzero(self.oob_predictions != y)\n",
    "                self.oob_score_ = errors / X.shape[0]\n",
    "        \n",
    "        if self.verbose >= 1:\n",
    "            print(\"##### Fitting is over #####\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.empty((X.shape[0], len(self.estimators)))\n",
    "        # predictions[i][j] = self.estimators[j].predict(X[i])\n",
    "        # TODO: More efficient method: only store new class and counters\n",
    "        for j, clf in enumerate(self.estimators):\n",
    "            predictions[:, j] = clf.predict(X)\n",
    "        \n",
    "        most_common = [Counter(line).most_common(1)[0][0] for line in predictions]\n",
    "        return most_common\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(self.predict(X), y)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 2\n",
    "Appliquez cette méthode sur des arbres de décisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n",
      "Classes in y: (array([0, 1, 2]), array([50, 50, 50]))\n",
      "Scores with bag_clf: 0.955555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Classes in y:\", np.unique(y, return_counts=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier, oob_score=True,\n",
    "                            n_estimators=100, verbose=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"Scores with bag_clf:\", bag_clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3\n",
    "Évaluez et commentez l’erreur en généralisation par rapport à un arbre unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the BaggingClassifier, OOB error: 0.05714285714285714\n",
      "For the DecisionTreeClassifier, OOB error: 0.0444444444444\n",
      "Best parameters found for the DecisionTreeClassifier: {'splitter': 'random', 'criterion': 'entropy', 'min_samples_split': 0.25074999999999997, 'max_depth': 35}\n"
     ]
    }
   ],
   "source": [
    "print(\"For the BaggingClassifier, OOB error:\", bag_clf.oob_score_)\n",
    "\n",
    "# Evaluating the OOB error on a decision tree:\n",
    "\n",
    "# Parameters for the Grid Search:\n",
    "params = {\"criterion\":(\"gini\", \"entropy\"),\n",
    "          \"splitter\":(\"best\", \"random\"),\n",
    "          \"max_depth\":np.arange(5, 51, 5),\n",
    "          \"min_samples_split\":np.linspace(1e-3, 1., 5)\n",
    "            }\n",
    "\n",
    "gridsearch = GridSearchCV(DecisionTreeClassifier(), params)\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print(\"For the DecisionTreeClassifier, OOB error:\", 1 - gridsearch.score(X_test, y_test))\n",
    "print(\"Best parameters found for the DecisionTreeClassifier:\", gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercice 2 Boosting\n",
    "### Question 1\n",
    "**Montrer que le coefficient de pondération des hypothèses vaut bien ce qu’il vaut.**\n",
    "\n",
    "Soit un problème d'apprentissage avec $((x_i, y_i)_{i \\in \\{1..n\\}}$, $x_i \\in X$, $y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "Après l'itération n° $m-1$, notre classifieur Boost est une combinaison linéaire de classifieurs faibles $h_i:X \\rightarrow \\{-1, +1\\}$ pondéré par les coefficients $a_i$:\n",
    "\n",
    "$C_{m-1}(x) = \\sum_{i=1}^{m-1}{a_i ⋅ h_i(x)}$.\n",
    "\n",
    "On veut trouver un nouvel estimateur faible $h_m$ pour avoir l'estimateur Boost à l'itération $m$.\n",
    "\n",
    "Soit l'erreur totale de $C_m$ : $E = \\sum_{i=1}^n{e^{-y_i ⋅ C_m(x_i)}}$, \n",
    "\n",
    "posons $w_i^1 = 1$ et $w_i^m = e^{-y_i ⋅ C_{m-1}(x_i)}$ pour $1 < m$, \n",
    "on a alors :\n",
    "\n",
    "$E = \\sum_{i=1}^n{e^{-y_i ⋅ C_m(x_i)}} \\\\\n",
    "E = \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}} + \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} \\\\\n",
    "E = \\sum_{i=1}^n{w_i^m ⋅ e^{-a_m}} + \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅(e^{a_m} - e^{-a_m})} $\n",
    "\n",
    "Ainsi l'estimateur faible $h_m$ qui minimise $E$ est celui qui minimise $\\sum_{y_i \\neq h_m(x_i)}{w_i^m}$, c'est-à-dire le classifieur qui fait le moins d'erreurs de classification, avec la pondération $w_i^m$.\n",
    "\n",
    "Après avoir trouvé ce classifieur $h_m$, il faut trouver le coefficient $a_m$, c'est le coefficient qui minimise $E$ :\n",
    "\n",
    "$\\frac {dE}{da_m} = \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} - \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}}$\n",
    "\n",
    "$\\frac {dE}{da_m} = 0 \\Leftrightarrow \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}} - \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}} = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{y_i \\neq h_m(x_i)}{w_i^m⋅e^{a_m}}  = \\sum_{y_i = h_m(x_i)}{w_i^m⋅e^{-a_m}}\\\\\n",
    "\\Leftrightarrow e^{2⋅a_m}⋅\\sum_{y_i \\neq h_m(x_i)}{w_i^m}  = \\sum_{y_i = h_m(x_i)}{w_i^m}\\\\\n",
    "\\Leftrightarrow 2⋅a_m = ln(\\frac {\\sum_{y_i = h_m(x_i)}{w_i^m}}{\\sum_{y_i \\neq h_m(x_i)}{w_i^m} })\\\\\n",
    "\\Leftrightarrow a_m = \\frac 1 2 ln(\\frac {\\sum_{y_i = h_m(x_i)}{w_i^m}}{\\sum_{y_i \\neq h_m(x_i)}{w_i^m} })$\n",
    "\n",
    "Ainsi on peut écrire l'erreur pondérée : \n",
    "\n",
    "$\\epsilon_m = \\frac {\\sum_{y_i \\neq h_m(x_i)}{w_i^m}} {\\sum_{i=1}^n{w_i^m}}$\n",
    "\n",
    "Et donc $a_m = \\frac 1 2 ln(\\frac {1-\\epsilon_m} {\\epsilon_m})$\n",
    "\n",
    "### Question 2\n",
    "Écrivez une fonction (ou une classe) implémentant la méthode AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Extracting the data #####\n",
      "Shape of X: (569, 30)\n",
      "Shape of y: (569,)\n",
      "Classes in y: (array([-1,  1]), array([212, 357]))\n",
      "##### Testing AdaBoost: #####\n",
      "Score on the test set: 0.947368421053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "class AdaBoostClassifier(ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator=None, n_estimators=50, learning_rate=1, verbose=False):\n",
    "        \"\"\" Create an AdaBoostClassifier.\n",
    "        :param base_estimator: The estimator used to fit the data. \n",
    "            Support for sample weighting is required\n",
    "        :param n_estimators: Number of used estimators\"\"\"\n",
    "        if base_estimator is None:\n",
    "            self.base_estimator = DecisionTreeClassifier()\n",
    "        else:\n",
    "            self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        sample_weight = np.array([1/y.shape[0]] * y.shape[0])\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        for it in range(self.n_estimators):\n",
    "            clf = clone(self.base_estimator)\n",
    "            clf.fit(X, y, sample_weight=sample_weight)\n",
    "            prediction = clf.predict(X)\n",
    "            misclass = np.arange(y.shape[0])[prediction != y]\n",
    "            error = sum(sample_weight[misclass])\n",
    "            \n",
    "            if error == 0:\n",
    "                error = 1/y.shape[0]\n",
    "            alpha = 1/2 * np.log((1-error)/error)\n",
    "            if self.verbose:\n",
    "                print(20*'-')\n",
    "                print(\"Weights:\", sample_weight[:20])\n",
    "                print(\"Misclassified:\", misclass[:10])\n",
    "                print(\"Summed error: %f, score: %f (%d misclassified)\" %(error, 1-error, len(misclass)))\n",
    "                print(\"Alpha:\", alpha)\n",
    "#                 print(\"Feature importance:\", clf.feature_importances_)\n",
    "                print(20*'-')\n",
    "            sample_weight = sample_weight * np.exp(-y * prediction * alpha)\n",
    "            sample_weight /= sum(sample_weight)\n",
    "            self.estimators.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for clf, alpha in zip(self.estimators, self.alphas):\n",
    "            if self.verbose:\n",
    "                print(clf.predict(X)[:10])\n",
    "            predictions += alpha * self.learning_rate * clf.predict(X)\n",
    "            \n",
    "        return np.sign(predictions)\n",
    "\n",
    "\n",
    "# Testing:\n",
    "print(\"##### Extracting the data #####\")\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "y = (y*2) - 1 # To bring y into {-1, +1}\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Classes in y:\", np.unique(y, return_counts=True))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        \n",
    "print(\"##### Testing AdaBoost: #####\")\n",
    "np.set_printoptions(precision=4)\n",
    "adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                              learning_rate=1,\n",
    "                              n_estimators=5, verbose=False)\n",
    "adaboost.fit(X_train, y_train)\n",
    "print(\"Score on the test set:\", accuracy_score(adaboost.predict(X_test), y_test))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 3\n",
    "Comment adapter un classifieur qui ne gère pas naturellement des pondérations pour qu’il puisse en tenir\n",
    "compte ?\n",
    "### Question 4\n",
    "Appliquez cette méthode sur des stumps (arbres de décision à un nœud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Finding the score of one stump #####\n",
      "Stump score: 0.906432748538\n",
      "##### Finding the score of a tree #####\n",
      "Tree score: 0.93567251462\n",
      "Finding n_estimators\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyUIppZRN/x/C3+OAHuIzjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb99502400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"##### Finding the score of one stump #####\")\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "stump_score = stump.score(X_test, y_test)\n",
    "print(\"Stump score:\", stump_score)\n",
    "\n",
    "print(\"##### Finding the score of a tree #####\")\n",
    "gridsearch = GridSearchCV(DecisionTreeClassifier(), \n",
    "                          param_grid={'max_depth':np.logspace(0.1, 2, 10, dtype=int),\n",
    "                                     'splitter':(\"best\", \"random\")})\n",
    "gridsearch.fit(X_train, y_train)\n",
    "tree_score = gridsearch.score(X_test, y_test)\n",
    "print(\"Tree score:\", tree_score)\n",
    "\n",
    "\n",
    "print(\"Finding n_estimators\")\n",
    "max_estimators = 50\n",
    "n_estimators_list = range(1, max_estimators, 2)\n",
    "scores_list = []\n",
    "for n_estimators in n_estimators_list:\n",
    "    adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                                  learning_rate=1,\n",
    "                                  n_estimators=n_estimators)\n",
    "    adaboost.fit(X_train, y_train)\n",
    "    scores_list.append(accuracy_score(adaboost.predict(X_test), y_test))\n",
    "\n",
    "# print(n_estimators_list)\n",
    "# print(scores_list)\n",
    "plt.plot(n_estimators_list, scores_list, label=\"AdaBoost\")\n",
    "plt.hlines(tree_score, xmin=0, xmax=max_estimators, label=\"Single tree\", colors=\"black\")\n",
    "plt.hlines(stump_score, xmin=0, xmax=max_estimators, label=\"Stump\", colors=\"r\")\n",
    "plt.xlabel(\"Number of stumps\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Question 5\n",
    "Tracez les courbes de l’erreur d’apprentissage et de l’erreur de test en fonction du nombre de classifieurs.\n",
    "1\n",
    "### Question 6\n",
    "Affichez les points associés aux poids les plus élevés (donc les plus difficiles à classer). Sont-ils bien classés\n",
    "désormais ? Étaient-ils bien classé avec un seul arbre de décision ? Avec le Bagging ?\n",
    "### Question 7\n",
    "Appliquez AdaBoost à des arbres de décision de profondeur plus grande.\n",
    "Comment se comporte la généralisation quand la profondeur des arbres augmente ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
