{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5 : Visualisation et recommandation\n",
    "\n",
    "## Exercice 1 : Réduction de dimension\n",
    "\n",
    "**Jeux de données :**\n",
    "- Swiss Roll http://dac.lip6.fr/master/wp-content/uploads/2017/09/swiss_roll.csv\n",
    "- Decathlon http://dac.lip6.fr/master/wp-content/uploads/2017/09/decathlon.csv (supprimer les\n",
    "variables qualitatives, et renverser les temps de façon à avoir une valeur élevée pour les meilleurs\n",
    "temps)\n",
    "- MNIST\n",
    "\n",
    "### Question 1\n",
    "**Implémentez le modèle t-SNE.**\n",
    "\n",
    "Implémentation inspirée de [Visualizing Data using t-SNE](http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "Dans mon implémentation, je calcule d'abord les distances entre les vecteurs $x_i$, puis toutes les probabilités conditionnelles et jointes pour les vecteurs $x_i$. Cela prend de la mémoire, mais accélère les calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n",
      "Classes in y: (array([0, 1, 2]), array([50, 50, 50]))\n",
      "Sample initial solution\n",
      "Iterate to find the best y\n",
      "[    0/500] Compute the gradient...\n",
      "Initializing joint probas matrix\n",
      "Initializing conditional probas matrix\n",
      "Trigger get_dist\n",
      "Initializing distances matrix\n",
      "Finished computing distances matrix\n",
      "Finished computing conditional probas matrix.\n",
      "Finished computing joint probas matrix.\n",
      "Gradient norm: 40356333.154892, incrementing y\n",
      "[    1/500] Compute the gradient...\n",
      "Gradient norm: 235.869132, incrementing y\n",
      "[    2/500] Compute the gradient...\n",
      "Gradient norm: 231.542531, incrementing y\n",
      "[    3/500] Compute the gradient...\n",
      "Gradient norm: 227.434264, incrementing y\n",
      "[    4/500] Compute the gradient...\n",
      "Gradient norm: 223.527287, incrementing y\n",
      "[    5/500] Compute the gradient...\n",
      "Gradient norm: 219.806310, incrementing y\n",
      "[    6/500] Compute the gradient...\n",
      "Gradient norm: 216.257577, incrementing y\n",
      "[    7/500] Compute the gradient...\n",
      "Gradient norm: 212.868676, incrementing y\n",
      "[    8/500] Compute the gradient...\n",
      "Gradient norm: 209.628380, incrementing y\n",
      "[    9/500] Compute the gradient...\n",
      "Gradient norm: 206.526502, incrementing y\n",
      "[   10/500] Compute the gradient...\n",
      "Gradient norm: 203.553785, incrementing y\n",
      "[   11/500] Compute the gradient...\n",
      "Gradient norm: 200.701790, incrementing y\n",
      "[   12/500] Compute the gradient...\n",
      "Gradient norm: 197.962810, incrementing y\n",
      "[   13/500] Compute the gradient...\n",
      "Gradient norm: 195.329792, incrementing y\n",
      "[   14/500] Compute the gradient...\n",
      "Gradient norm: 192.796268, incrementing y\n",
      "[   15/500] Compute the gradient...\n",
      "Gradient norm: 190.356296, incrementing y\n",
      "[   16/500] Compute the gradient...\n",
      "Gradient norm: 188.004406, incrementing y\n",
      "[   17/500] Compute the gradient...\n",
      "Gradient norm: 185.735555, incrementing y\n",
      "[   18/500] Compute the gradient...\n",
      "Gradient norm: 183.545083, incrementing y\n",
      "[   19/500] Compute the gradient...\n",
      "Gradient norm: 181.428683, incrementing y\n",
      "[   20/500] Compute the gradient...\n",
      "Gradient norm: 179.382361, incrementing y\n",
      "[   21/500] Compute the gradient...\n",
      "Gradient norm: 177.402412, incrementing y\n",
      "[   22/500] Compute the gradient...\n",
      "Gradient norm: 175.485394, incrementing y\n",
      "[   23/500] Compute the gradient...\n",
      "Gradient norm: 173.628102, incrementing y\n",
      "[   24/500] Compute the gradient...\n",
      "Gradient norm: 171.827554, incrementing y\n",
      "[   25/500] Compute the gradient...\n",
      "Gradient norm: 170.080963, incrementing y\n",
      "[   26/500] Compute the gradient...\n",
      "Gradient norm: 168.385729, incrementing y\n",
      "[   27/500] Compute the gradient...\n",
      "Gradient norm: 166.739420, incrementing y\n",
      "[   28/500] Compute the gradient...\n",
      "Gradient norm: 165.139757, incrementing y\n",
      "[   29/500] Compute the gradient...\n",
      "Gradient norm: 163.584607, incrementing y\n",
      "[   30/500] Compute the gradient...\n",
      "Gradient norm: 162.071966, incrementing y\n",
      "[   31/500] Compute the gradient...\n",
      "Gradient norm: 160.599952, incrementing y\n",
      "[   32/500] Compute the gradient...\n",
      "Gradient norm: 159.166798, incrementing y\n",
      "[   33/500] Compute the gradient...\n",
      "Gradient norm: 157.770837, incrementing y\n",
      "[   34/500] Compute the gradient...\n",
      "Gradient norm: 156.410501, incrementing y\n",
      "[   35/500] Compute the gradient...\n",
      "Gradient norm: 155.084312, incrementing y\n",
      "[   36/500] Compute the gradient...\n",
      "Gradient norm: 153.790873, incrementing y\n",
      "[   37/500] Compute the gradient...\n",
      "Gradient norm: 152.528866, incrementing y\n",
      "[   38/500] Compute the gradient...\n",
      "Gradient norm: 151.297045, incrementing y\n",
      "[   39/500] Compute the gradient...\n",
      "Gradient norm: 150.094229, incrementing y\n",
      "[   40/500] Compute the gradient...\n",
      "Gradient norm: 148.919303, incrementing y\n",
      "[   41/500] Compute the gradient...\n",
      "Gradient norm: 147.771208, incrementing y\n",
      "[   42/500] Compute the gradient...\n",
      "Gradient norm: 146.648940, incrementing y\n",
      "[   43/500] Compute the gradient...\n",
      "Gradient norm: 145.551548, incrementing y\n",
      "[   44/500] Compute the gradient...\n",
      "Gradient norm: 144.478125, incrementing y\n",
      "[   45/500] Compute the gradient...\n",
      "Gradient norm: 143.427812, incrementing y\n",
      "[   46/500] Compute the gradient...\n",
      "Gradient norm: 142.399792, incrementing y\n",
      "[   47/500] Compute the gradient...\n",
      "Gradient norm: 141.393287, incrementing y\n",
      "[   48/500] Compute the gradient...\n",
      "Gradient norm: 140.407554, incrementing y\n",
      "[   49/500] Compute the gradient...\n",
      "Gradient norm: 139.441889, incrementing y\n",
      "[   50/500] Compute the gradient...\n",
      "Gradient norm: 138.495618, incrementing y\n",
      "[   51/500] Compute the gradient...\n",
      "Gradient norm: 137.568098, incrementing y\n",
      "[   52/500] Compute the gradient...\n",
      "Gradient norm: 136.658716, incrementing y\n",
      "[   53/500] Compute the gradient...\n",
      "Gradient norm: 135.766887, incrementing y\n",
      "[   54/500] Compute the gradient...\n",
      "Gradient norm: 134.892051, incrementing y\n",
      "[   55/500] Compute the gradient...\n",
      "Gradient norm: 134.033672, incrementing y\n",
      "[   56/500] Compute the gradient...\n",
      "Gradient norm: 133.191238, incrementing y\n",
      "[   57/500] Compute the gradient...\n",
      "Gradient norm: 132.364260, incrementing y\n",
      "[   58/500] Compute the gradient...\n",
      "Gradient norm: 131.552268, incrementing y\n",
      "[   59/500] Compute the gradient...\n",
      "Gradient norm: 130.754811, incrementing y\n",
      "[   60/500] Compute the gradient...\n",
      "Gradient norm: 129.971459, incrementing y\n",
      "[   61/500] Compute the gradient...\n",
      "Gradient norm: 129.201799, incrementing y\n",
      "[   62/500] Compute the gradient...\n",
      "Gradient norm: 128.445432, incrementing y\n",
      "[   63/500] Compute the gradient...\n",
      "Gradient norm: 127.701979, incrementing y\n",
      "[   64/500] Compute the gradient...\n",
      "Gradient norm: 126.971073, incrementing y\n",
      "[   65/500] Compute the gradient...\n",
      "Gradient norm: 126.252363, incrementing y\n",
      "[   66/500] Compute the gradient...\n",
      "Gradient norm: 125.545510, incrementing y\n",
      "[   67/500] Compute the gradient...\n",
      "Gradient norm: 124.850190, incrementing y\n",
      "[   68/500] Compute the gradient...\n",
      "Gradient norm: 124.166090, incrementing y\n",
      "[   69/500] Compute the gradient...\n",
      "Gradient norm: 123.492909, incrementing y\n",
      "[   70/500] Compute the gradient...\n",
      "Gradient norm: 122.830357, incrementing y\n",
      "[   71/500] Compute the gradient...\n",
      "Gradient norm: 122.178156, incrementing y\n",
      "[   72/500] Compute the gradient...\n",
      "Gradient norm: 121.536035, incrementing y\n",
      "[   73/500] Compute the gradient...\n",
      "Gradient norm: 120.903735, incrementing y\n",
      "[   74/500] Compute the gradient...\n",
      "Gradient norm: 120.281008, incrementing y\n",
      "[   75/500] Compute the gradient...\n",
      "Gradient norm: 119.667610, incrementing y\n",
      "[   76/500] Compute the gradient...\n",
      "Gradient norm: 119.063311, incrementing y\n",
      "[   77/500] Compute the gradient...\n",
      "Gradient norm: 118.467884, incrementing y\n",
      "[   78/500] Compute the gradient...\n",
      "Gradient norm: 117.881114, incrementing y\n",
      "[   79/500] Compute the gradient...\n",
      "Gradient norm: 117.302789, incrementing y\n",
      "[   80/500] Compute the gradient...\n",
      "Gradient norm: 116.732709, incrementing y\n",
      "[   81/500] Compute the gradient...\n",
      "Gradient norm: 116.170676, incrementing y\n",
      "[   82/500] Compute the gradient...\n",
      "Gradient norm: 115.616502, incrementing y\n",
      "[   83/500] Compute the gradient...\n",
      "Gradient norm: 115.070004, incrementing y\n",
      "[   84/500] Compute the gradient...\n",
      "Gradient norm: 114.531004, incrementing y\n",
      "[   85/500] Compute the gradient...\n",
      "Gradient norm: 113.999331, incrementing y\n",
      "[   86/500] Compute the gradient...\n",
      "Gradient norm: 113.474818, incrementing y\n",
      "[   87/500] Compute the gradient...\n",
      "Gradient norm: 112.957306, incrementing y\n",
      "[   88/500] Compute the gradient...\n",
      "Gradient norm: 112.446638, incrementing y\n",
      "[   89/500] Compute the gradient...\n",
      "Gradient norm: 111.942664, incrementing y\n",
      "[   90/500] Compute the gradient...\n",
      "Gradient norm: 111.445236, incrementing y\n",
      "[   91/500] Compute the gradient...\n",
      "Gradient norm: 110.954214, incrementing y\n",
      "[   92/500] Compute the gradient...\n",
      "Gradient norm: 110.469460, incrementing y\n",
      "[   93/500] Compute the gradient...\n",
      "Gradient norm: 109.990840, incrementing y\n",
      "[   94/500] Compute the gradient...\n",
      "Gradient norm: 109.518225, incrementing y\n",
      "[   95/500] Compute the gradient...\n",
      "Gradient norm: 109.051489, incrementing y\n",
      "[   96/500] Compute the gradient...\n",
      "Gradient norm: 108.590511, incrementing y\n",
      "[   97/500] Compute the gradient...\n",
      "Gradient norm: 108.135172, incrementing y\n",
      "[   98/500] Compute the gradient...\n",
      "Gradient norm: 107.685357, incrementing y\n",
      "[   99/500] Compute the gradient...\n",
      "Gradient norm: 107.240954, incrementing y\n",
      "[  100/500] Compute the gradient...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm: 106.801855, incrementing y\n",
      "[  101/500] Compute the gradient...\n",
      "Gradient norm: 106.367954, incrementing y\n",
      "[  102/500] Compute the gradient...\n",
      "Gradient norm: 105.939149, incrementing y\n",
      "[  103/500] Compute the gradient...\n",
      "Gradient norm: 105.515340, incrementing y\n",
      "[  104/500] Compute the gradient...\n",
      "Gradient norm: 105.096430, incrementing y\n",
      "[  105/500] Compute the gradient...\n",
      "Gradient norm: 104.682324, incrementing y\n",
      "[  106/500] Compute the gradient...\n",
      "Gradient norm: 104.272931, incrementing y\n",
      "[  107/500] Compute the gradient...\n",
      "Gradient norm: 103.868161, incrementing y\n",
      "[  108/500] Compute the gradient...\n",
      "Gradient norm: 103.467928, incrementing y\n",
      "[  109/500] Compute the gradient...\n",
      "Gradient norm: 103.072146, incrementing y\n",
      "[  110/500] Compute the gradient...\n",
      "Gradient norm: 102.680733, incrementing y\n",
      "[  111/500] Compute the gradient...\n",
      "Gradient norm: 102.293609, incrementing y\n",
      "[  112/500] Compute the gradient...\n",
      "Gradient norm: 101.910696, incrementing y\n",
      "[  113/500] Compute the gradient...\n",
      "Gradient norm: 101.531917, incrementing y\n",
      "[  114/500] Compute the gradient...\n",
      "Gradient norm: 101.157198, incrementing y\n",
      "[  115/500] Compute the gradient...\n",
      "Gradient norm: 100.786467, incrementing y\n",
      "[  116/500] Compute the gradient...\n",
      "Gradient norm: 100.419652, incrementing y\n",
      "[  117/500] Compute the gradient...\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse, optimize\n",
    "import itertools\n",
    "from sklearn import datasets\n",
    "from multiprocessing import Pool\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class TSNE():\n",
    "    \"\"\" Implementation t-SNE to reduce dimension \"\"\"   \n",
    "    def __init__(self, n_components=2, perplexity=30.0, n_iter=1000, learning_rate=200.0, n_jobs=4):\n",
    "        \"\"\" Constructor\n",
    "        :param n_components: Number of dimensions in the output space\n",
    "        param perplexity: Related to the number of nearest neighbours used\n",
    "        param n_iter: Number of iterations\n",
    "        param learning_rate: Coefficient that multiplies the gradient at each iteration\n",
    "        param n_jobs: Number of processes spawn in the computations\"\"\"\n",
    "        # Parameters:\n",
    "        self.n_components  = n_components\n",
    "        self.perplexity    = perplexity\n",
    "        self.n_iter        = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_jobs        = n_jobs\n",
    "        \n",
    "        # Memoization:\n",
    "        self.X         = None # Used to have a simple function find_sigma\n",
    "        self.distances = None # squared distances, upper triangular matrix\n",
    "        self.joint_p   = None # self.joint_p[i,j] = p(i, j)\n",
    "        self.cond_p    = None # self.cond_p[i,j] = p(j|i), upper triang. matrix\n",
    "        self.sigma     = None # Array of sigmas_i\n",
    "    \n",
    "    def get_dist(self, X, i, j):\n",
    "        \"\"\" Return the squared distance between two vectors. \"\"\"\n",
    "        if self.distances is None:\n",
    "            print(\"Initializing distances matrix\")\n",
    "            self.distances = np.zeros((X.shape[0], X.shape[0]))\n",
    "            for k,l in itertools.combinations(range(X.shape[0]), 2):\n",
    "                self.distances[k,l] = sum((X[k] - X[l])**2)\n",
    "            self.distances = sparse.triu(self.distances, format=\"lil\")\n",
    "            print(\"Finished computing distances matrix\")\n",
    "        \n",
    "        if i>j:\n",
    "            i,j = j,i\n",
    "        return self.distances[i,j]\n",
    "        \n",
    "    def get_cond_p(self, i, j):\n",
    "        \"\"\" Return p(j|i). Find the sigma that fit the desired perplexity.\n",
    "        Create the arrays of conditional probas and sigmas if they don't exist\"\"\"\n",
    "        # Create the matrix if it doesn't exist:\n",
    "        if self.cond_p is None:\n",
    "            print(\"Initializing conditional probas matrix\")\n",
    "            N = X.shape[0] # number of vectors\n",
    "            print(\"Trigger get_dist\")\n",
    "            self.get_dist(X, 0, 1)\n",
    "            self.cond_p = np.zeros((N, N))\n",
    "            pool = Pool(processes=self.n_jobs)\n",
    "            self.sigma = pool.map(self.find_sigma, range(N))\n",
    "#             print(\"Sigmas found: \", self.sigma)\n",
    "            for k in range(N):\n",
    "                # Compute all conditional probabilities\n",
    "                denom = sum([np.exp(-self.get_dist(X, k, m)/(2 * self.sigma[k]**2)) for m in range(N)\n",
    "                            if m != k])\n",
    "                for l in range(N):\n",
    "                    if l == k:\n",
    "                        self.cond_p[k, l] = 0\n",
    "                    else:\n",
    "                        numerator = np.exp(-self.get_dist(X, i, j)/(2 * self.sigma[k]**2))\n",
    "                        self.cond_p[k,l] = numerator/denom\n",
    "\n",
    "            print(\"Finished computing conditional probas matrix.\")\n",
    "        \n",
    "        return self.cond_p[i, j]\n",
    "    \n",
    "    def compute_conditional_p(self, X, i, j, sigma_i):\n",
    "        \"\"\"Returns p(j|i), used to find the sigmas that fit the perplexity\"\"\"\n",
    "        if i == j:\n",
    "            return 0\n",
    "        numerator = np.exp(-self.get_dist(X, i, j)/(2 * sigma_i**2))\n",
    "        denom = sum([np.exp(-self.get_dist(X, i, k)/(2 * sigma_i**2)) for k in range(X.shape[0])\n",
    "                    if k != i])\n",
    "        return numerator/denom    \n",
    "    \n",
    "    def find_sigma(self, i, max_iter=100):\n",
    "        \"\"\"Find sigma component that gives the wanted perplexity\"\"\"\n",
    "#         print(\"Find sigma component #%d to achieve a perplexity of %f\" % (i, self.perplexity))\n",
    "#         print(\"Find sigmin...\")\n",
    "        sigmin = 1\n",
    "        n = 0\n",
    "        while n < max_iter and self.compute_perplexity(X, i, sigmin) > self.perplexity:\n",
    "            sigmin /= 2\n",
    "            n += 1\n",
    "#         print(\"Found: %f\" % sigmin)\n",
    "            \n",
    "#         print(\"Find sigmax...\")\n",
    "        sigmax = 10\n",
    "        n = 0\n",
    "        while n < max_iter and self.compute_perplexity(X, i, sigmax) < self.perplexity:\n",
    "            sigmax *= 2\n",
    "            n += 1\n",
    "#         print(\"Found: %f\" % sigmax)\n",
    "#         print(\"Calling optimize.bisect\")\n",
    "        f = lambda s: self.compute_perplexity(X, i, s) - self.perplexity\n",
    "        sigma_i = optimize.bisect(f, sigmin, sigmax, maxiter=max_iter, rtol=1e-5)\n",
    "#         print(\"sigma_i = %f\" % sigma_i)\n",
    "        return sigma_i\n",
    "\n",
    "    def compute_perplexity(self, X, i, sigma_i):\n",
    "        \"\"\" Returns the perplexity\"\"\"\n",
    "#         print(\"Compute perplexity(i=%d, sigma=%f)\" % (i, sigma_i))\n",
    "        N = X.shape[0]\n",
    "        h = 0 # Shannon entropy of P_i\n",
    "        denom = sum([np.exp(-self.get_dist(X, i, k)/(2 * sigma_i**2)) for k in range(X.shape[0])\n",
    "                    if k != i])\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                pass\n",
    "            numerator = np.exp(-self.get_dist(X, i, j)/(2 * sigma_i**2))\n",
    "            p = numerator/denom    \n",
    "            if p != 0:\n",
    "                h -= p * np.log2(p)\n",
    "#             print(\"j=%d, p=%f, h=%f\" % (j, p, h))\n",
    "        perp = 2**h\n",
    "#         print(\"perplexity = %f\" % (perp))\n",
    "        return perp\n",
    "        \n",
    "    def get_joint_p(self, X, i, j):\n",
    "        \"\"\" Return p_{ij}\"\"\"\n",
    "        N = X.shape[0]        \n",
    "        if self.joint_p is None:\n",
    "            print(\"Initializing joint probas matrix\")\n",
    "            self.joint_p = np.zeros((N,N))\n",
    "            for k,l in itertools.combinations_with_replacement(range(N), 2):\n",
    "                self.joint_p[k,l] = (self.get_cond_p(k, l) + self.get_cond_p(l, k)) / (2*N)\n",
    "            self.joint_p = sparse.triu(self.joint_p, format=\"lil\")\n",
    "            print(\"Finished computing joint probas matrix.\")\n",
    "        \n",
    "        if i>j:\n",
    "            i,j = j,i\n",
    "        return self.joint_p[i,j]\n",
    "    \n",
    "    def compute_joint_q(self, y, i, j):\n",
    "        numerator = sum([(1+ sum((y[k] - y[l])**2)) \n",
    "                        for k,l in itertools.product(range(y.shape[0]), repeat=2)\n",
    "                        if k != l])\n",
    "        denom = 1 + sum((y[i] - y[j])**2)\n",
    "        return numerator/denom\n",
    "    \n",
    "    def gradient_cost(self, X, y, i=None):\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        numerator = sum([(1+ sum((y[k] - y[l])**2)) \n",
    "                        for k,l in itertools.product(range(y.shape[0]), repeat=2)\n",
    "                        if k != l])\n",
    "        \n",
    "        def grad_i(i):\n",
    "            s = 0\n",
    "            for j in range(N):\n",
    "                denom = 1 + sum((y[i] - y[j])**2)\n",
    "                s += (self.get_joint_p(X, i, j) -  (numerator/denom)) * \\\n",
    "                     (y[i] - y[j]) / (1 + sum((y[i]-y[j])**2))\n",
    "            return 4*s\n",
    "\n",
    "        if i is not None:\n",
    "            return grad_i(i)\n",
    "        else :\n",
    "            apply_results = np.zeros_like(y)\n",
    "            for i in range(N):\n",
    "                apply_results[i] = grad_i(i)\n",
    "            return apply_results\n",
    "    \n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\" Return the low-dimensional representation of X.\"\"\"\n",
    "        # Reset the matrices:\n",
    "        self.X         = X\n",
    "        self.joint_p   = None\n",
    "        self.distances = None\n",
    "        self.cond_p    = None\n",
    "        self.sigma     = None\n",
    "        # Sample initial solution y from a gaussian distribution\n",
    "        print(\"Sample initial solution\")\n",
    "        y = np.random.random((X.shape[0], self.n_components))\n",
    "        print(\"Iterate to find the best y\")\n",
    "        for t in range(self.n_iter):\n",
    "            print(\"[%5d/%d] Compute the gradient...\" % (t, self.n_iter))\n",
    "            grad = self.gradient_cost(X, y)\n",
    "            print(\"Gradient norm: %f, incrementing y\" % np.linalg.norm(grad))\n",
    "            y -= self.learning_rate * grad\n",
    "        return y\n",
    "    \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:], iris.target[:]\n",
    "X,y = shuffle(X, y)\n",
    "\n",
    "# X,y = X[:30], y[:30]\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Classes in y:\", np.unique(y, return_counts=True))\n",
    "\n",
    "tsne = TSNE(n_components = 2, perplexity=30, n_iter=500, n_jobs=10)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "print(\"Shape of X_tsne:\", X_tsne.shape)\n",
    "\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Construisez des visualisation (en 2D et en 3D) des jeux de données proposés (ou autres). \n",
    "\n",
    "Comparez avec d’autres méthodes de réduction de dimension, linéaires et non-linéaires (voir http://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Classes in y:\", np.unique(y, return_counts=True))\n",
    "\n",
    "skl_tsne = manifold.TSNE(n_components = 2)\n",
    "X_tsne = skl_tsne.fit_transform(X, y)\n",
    "print(\"Shape of X_tsne:\", X_tsne.shape)\n",
    "\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question 3\n",
    "Pour Isomap et LLE, manipulez le paramètre contrôlant la taille du voisinage pour la construction du\n",
    "graphe.\n",
    "Exercice 2 Recommandation\n",
    "Jeu de données MovieLens http://files.grouplens.org/datasets/movielens/ :\n",
    "- 100k http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "- 1M http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "Question 1\n",
    "Construisez une visualisation avec t-SNE (on peut utiliser metric=precomputed pour donner une matrice\n",
    "de distance au lieu des coordonnées des points en entrée de la fonction sklearn).\n",
    "1\n",
    "Question 2\n",
    "Implémentez un modèle de collaborative filtering avec :\n",
    "- descente de gradient stochastique\n",
    "- distance L2\n",
    "- régularisation L2\n",
    "- sans puis avec biais\n",
    "Question 3\n",
    "Évaluez vos modèles sur MovieLens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
