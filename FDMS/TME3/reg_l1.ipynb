{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TME 3 - RÃ©gularisation L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (569, 30)\n",
      "shape of Y: (569,)\n",
      "classes in Y:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "print(\"shape of X:\", X.shape)\n",
    "y = cancer.target\n",
    "print(\"shape of Y:\", y.shape)\n",
    "print(\"classes in Y: \", np.unique(y))\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "# Restricting data\n",
    "X = X[:100,:10]\n",
    "y = y[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient:\n",
      "\n",
      "### Iteration 0 ###\n",
      "approx_fprime: [  2.88e+03   3.72e+03   2.63e+04   6.51e+05   1.64e+01   2.03e+01\n",
      "   1.99e+01   1.12e+01   3.07e+01   1.05e+01]\n",
      "gradient_loss: [  2.68e+03   3.34e+03   1.76e+04   1.40e+05   1.63e+01   2.03e+01\n",
      "   1.98e+01   1.12e+01   3.07e+01   1.05e+01]\n",
      "check: 0.00765599646048\n",
      "\n",
      "### Iteration 1 ###\n",
      "approx_fprime: [  2.10e+03   2.80e+03   2.12e+04   6.08e+05   1.19e+01   1.44e+01\n",
      "   1.39e+01   7.76e+00   2.22e+01   7.60e+00]\n",
      "gradient_loss: [  1.90e+03   2.42e+03   1.24e+04   9.74e+04   1.19e+01   1.44e+01\n",
      "   1.39e+01   7.75e+00   2.21e+01   7.60e+00]\n",
      "check: 0.00764845078973\n",
      "\n",
      "### Iteration 2 ###\n",
      "approx_fprime: [ -2.81e+03  -3.40e+03  -1.10e+04   3.54e+05  -1.86e+01  -2.26e+01\n",
      "  -2.19e+01  -1.23e+01  -3.48e+01  -1.19e+01]\n",
      "gradient_loss: [ -3.02e+03  -3.79e+03  -1.98e+04  -1.56e+05  -1.87e+01  -2.27e+01\n",
      "  -2.19e+01  -1.24e+01  -3.49e+01  -1.19e+01]\n",
      "check: 0.00755295909611\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import approx_fprime, check_grad, minimize\n",
    "\n",
    "def decision(theta, X):\n",
    "    return np.sign(X.dot(theta)) / 2 + 0.5\n",
    "\n",
    "def loss(theta, X, y, pen):\n",
    "    mse = ((y - X.dot(theta))**2).mean()\n",
    "    return mse + pen * sum([np.abs(t) for t in theta])\n",
    "\n",
    "def gradient_loss(theta, X, y, pen):\n",
    "    gradient = np.array(2 * X.T .dot(X.dot(theta) - y)) / X.shape[0] + pen * np.sign(theta)\n",
    "    return gradient\n",
    "\n",
    "print(\"Checking gradient:\")\n",
    "for i in range(3):\n",
    "    print(\"\\n### Iteration %d ###\" % i)\n",
    "    theta = np.random.random(X[0].shape) - 0.5\n",
    "    pen = np.random.random() / 10\n",
    "    func = lambda t:loss(t, X, y, pen)\n",
    "    grad = lambda t:gradient_loss(t, X, y, pen)\n",
    "    print(\"approx_fprime:\", approx_fprime(theta, func, epsilon=1))\n",
    "    print(\"gradient_loss:\", grad(theta))\n",
    "    print(\"check:\", check_grad(func, grad, theta))\n",
    "\n",
    "\n",
    "def gradient_l1(theta, X, y, max_iter=100, step=1e-1, pen=0.1, verbose=0, plot=False):\n",
    "    \"\"\" Performs gradient clipping to find the best theta\n",
    "    :param:\n",
    "    \n",
    "    :return:\"\"\"\n",
    "    \n",
    "    (l, n) = X.shape\n",
    "    losses = []\n",
    "    for it in range(max_iter):\n",
    "        if verbose >= 1:\n",
    "            print(\"###########  Step\", it, \": #################\")\n",
    "        for i in range(l):\n",
    "            idx = np.random.randint(0, l)\n",
    "            if verbose >= 2:\n",
    "                print(\"========= i = %d, idx = %d ===========\" % (i, idx))\n",
    "            local_loss = lambda t: loss(t, np.array([X[idx]]), np.array([y[idx]]), pen)\n",
    "#             grad = approx_fprime(theta, local_loss, epsilon=step)\n",
    "            grad = gradient_loss(theta, X, y, pen)\n",
    "            theta_prime = theta - step * grad\n",
    "            \n",
    "            if verbose >= 2:\n",
    "#                 print(\"Random sample:\", X[idx])\n",
    "                print(\"Gradient:     \",grad)\n",
    "                print(\"Theta before: \", theta)\n",
    "            theta_prime[theta * theta_prime < 0] = 0\n",
    "            theta = theta_prime\n",
    "            if verbose >= 2:\n",
    "                print(\"Theta after:  \", theta)\n",
    "                \n",
    "        general_loss = loss(theta, X, y, pen)\n",
    "        losses.append(general_loss)\n",
    "        if verbose >= 1:\n",
    "            print(\"L = %f\" % (general_loss))\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Evolution of loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta with minimize():             [  1.51e-01  -1.02e-02  -7.44e-03  -8.91e-04  -2.68e-07  -7.21e-08\n",
      "  -2.11e+00  -6.08e+00  -5.29e-10   6.93e+00]\n",
      "Searching best parameters\n",
      "0/100\n",
      "1/100\n",
      "2/100\n",
      "3/100\n",
      "4/100\n",
      "5/100\n",
      "6/100\n",
      "7/100\n",
      "8/100\n",
      "9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:50: RuntimeWarning: overflow encountered in multiply\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in square\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:11: RuntimeWarning: overflow encountered in multiply\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:50: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:50: RuntimeWarning: invalid value encountered in less\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:44: RuntimeWarning: invalid value encountered in subtract\n",
      "/home/loic/.local/lib/python3.5/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in absolute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/100\n",
      "11/100\n",
      "12/100\n",
      "13/100\n",
      "14/100\n",
      "15/100\n",
      "16/100\n",
      "17/100\n",
      "18/100\n",
      "19/100\n",
      "20/100\n",
      "21/100\n",
      "22/100\n",
      "23/100\n",
      "24/100\n",
      "25/100\n",
      "26/100\n",
      "27/100\n",
      "28/100\n",
      "29/100\n",
      "30/100\n",
      "31/100\n",
      "32/100\n",
      "33/100\n",
      "34/100\n",
      "35/100\n",
      "36/100\n",
      "37/100\n",
      "38/100\n",
      "39/100\n",
      "40/100\n",
      "41/100\n",
      "42/100\n",
      "43/100\n",
      "44/100\n",
      "45/100\n",
      "46/100\n",
      "47/100\n",
      "48/100\n",
      "49/100\n",
      "50/100\n",
      "51/100\n",
      "52/100\n",
      "53/100\n",
      "54/100\n",
      "55/100\n",
      "56/100\n",
      "57/100\n",
      "58/100\n",
      "59/100\n",
      "60/100\n",
      "61/100\n",
      "62/100\n",
      "63/100\n",
      "64/100\n",
      "65/100\n",
      "66/100\n",
      "67/100\n",
      "68/100\n",
      "69/100\n",
      "70/100\n",
      "71/100\n",
      "72/100\n",
      "73/100\n",
      "74/100\n",
      "75/100\n",
      "76/100\n",
      "77/100\n",
      "78/100\n",
      "79/100\n",
      "80/100\n",
      "81/100\n",
      "82/100\n",
      "83/100\n",
      "84/100\n",
      "85/100\n",
      "86/100\n",
      "87/100\n",
      "88/100\n",
      "89/100\n",
      "90/100\n",
      "91/100\n",
      "92/100\n",
      "93/100\n",
      "94/100\n",
      "95/100\n",
      "96/100\n",
      "97/100\n",
      "98/100\n",
      "99/100\n"
     ]
    }
   ],
   "source": [
    "# Optimization parameters\n",
    "theta_init = np.random.random(X[0].shape)\n",
    "max_iter = 200\n",
    "step     = 1e-3\n",
    "pen      = 1e-3\n",
    "\n",
    "# Find theta \n",
    "np.set_printoptions(precision=2, threshold=10)\n",
    "theta_minimize = minimize(lambda t:loss(t, X, y, pen), theta_init).x\n",
    "print(\"theta with minimize():            \", theta_minimize)\n",
    "\n",
    "params = {\"step\": np.logspace(-6, 1, 10),\n",
    "          \"pen\" : np.logspace(-6, 1, 10)}\n",
    "combinations = functools.reduce(operator.mul, [len(elt) for elt in params.values()])\n",
    "best_params = {\"step\": params[\"step\"][0],  \"pen\": params[\"pen\"][0]}\n",
    "best_theta = gradient_l1(theta_init, X, y, max_iter=max_iter, step=best_params[\"step\"], pen=best_params[\"pen\"])\n",
    "print(\"Searching best parameters\")\n",
    "for i, (it_step, it_pen) in enumerate(itertools.product(params[\"step\"], params[\"pen\"])):\n",
    "    theta = gradient_l1(theta_init, X, y, max_iter=max_iter, step=it_step, pen=it_pen)\n",
    "    print(\"%d/%d\" % (i, combinations) )\n",
    "    if np.linalg.norm(theta - theta_minimize) < np.linalg.norm(best_theta - theta_minimize):\n",
    "        best_theta = theta\n",
    "        best_params = {\"step\": it_step, \"pen\": it_pen}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "{'pen': 9.9999999999999995e-07, 'step': 0.00021544346900318845}\n",
      "[  3.42e-03   4.96e-03   2.20e-02   1.33e-01   2.52e-05   2.15e-05\n",
      "   1.24e-05   7.11e-06   4.77e-05   1.71e-05]\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")\n",
    "print(best_params)\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\" Custom linear classifier \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter=1000, step=1e-2, pen=1e-2, verbose=0):\n",
    "        self.theta= 0\n",
    "        self.max_iter = max_iter\n",
    "        self.step = step\n",
    "        self.pen = pen\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = gradient_l1(np.ones_like(X[0]), X, y, \n",
    "                                 max_iter=self.max_iter, \n",
    "                                 step=self.step, \n",
    "                                 pen=self.pen,\n",
    "                                 verbose=self.verbose)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return decision(theta, X)\n",
    "\n",
    "    def get_theta(self):\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found: [-0.01 -0.01 -0.01 ..., -0.   -0.01 -0.01]\n",
      "KFold:\n",
      "Y_Pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "Y_reel:  [0 0 1 ..., 1 1 0]\n",
      "Score:  0.56\n",
      "Errors:  [0 0 0 ..., 0 0 0]\n",
      "Theta found: [-0.01 -0.01 -0.01 ..., -0.01 -0.01 -0.01]\n",
      "KFold:\n",
      "Y_Pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "Y_reel:  [0 0 1 ..., 0 0 1]\n",
      "Score:  0.44\n",
      "Errors:  [0 0 0 ..., 0 0 0]\n",
      "Theta found: [-0.01 -0.01 -0.01 ..., -0.01 -0.01 -0.01]\n",
      "KFold:\n",
      "Y_Pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "Y_reel:  [1 1 0 ..., 1 0 1]\n",
      "Score:  0.64\n",
      "Errors:  [0 0 0 ..., 0 0 0]\n",
      "Theta found: [-0.01 -0.01 -0.01 ..., -0.01 -0.01 -0.01]\n",
      "KFold:\n",
      "Y_Pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "Y_reel:  [1 0 1 ..., 0 1 0]\n",
      "Score:  0.6\n",
      "Errors:  [0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = y[train_index], y[test_index]\n",
    "    clf = LinearClassifier(max_iter=max_iter, step=step, pen=pen, verbose=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(\"Theta found:\", clf.get_theta())\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"KFold:\")\n",
    "    print(\"Y_Pred: \", pred)\n",
    "    print(\"Y_reel: \", Y_test)\n",
    "    print(\"Score: \", accuracy_score(pred, Y_test))\n",
    "    print(\"Errors: \", Y_test[pred != Y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
