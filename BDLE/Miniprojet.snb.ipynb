{
  "metadata" : {
    "id" : "b3014f52-d363-4eb2-85ac-858b8b9d92bc",
    "name" : "Miniprojet",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "id" : "4530ADB7AE50422F8329BDDF5E8F1D89"
      },
      "cell_type" : "markdown",
      "source" : "# Partie principale   \n\nDatasets à utiliser : Yago\n\n**Attention : Dataset dans `/tmp/BDLE_3774877/dataset`**\n\n````\nmkdir -p /tmp/BDLE_3774877/dataset\ncd /tmp/BDLE_3774877/dataset\ntar zxvf /Infos/bd/spark/dataset/freebase/freebase_snippet_surls.tgz\ntar zxvf /Infos/bd/spark/dataset/yago/yagoFacts5M.tgz\ncp  /Infos/bd/spark/dataset/yago/yagoMiniSample.txt /tmp/BDLE_3774877/dataset\n````"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "02E0EF8038DF4EC599208DB19F9F40A0"
      },
      "cell_type" : "code",
      "source" : [
        "// Extraction des données\n",
        "\n",
        "case class Triple(sujet: String, prop: String, objet: String)\n",
        "\n",
        "val dataset = \"/tmp/BDLE_3774877/dataset/\"\n",
        "val yagoFile = dataset + \"yagoFacts5M.txt\""
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "defined class Triple\ndataset: String = /tmp/BDLE_3774877/dataset/\nyagoFile: String = /tmp/BDLE_3774877/dataset/yagoFacts5M.txt\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 1,
          "time" : "Took: 0.768s, at 2017-11-09 13:12"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1658543935-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "DBFB5BEC81E1453887B0F7B309E0B5DD"
      },
      "cell_type" : "code",
      "source" : [
        "val yago = sc.textFile(yagoFile).\n",
        "  map(ligne => ligne.split(\"\\\\t\")).coalesce(8).\n",
        "  map(tab => Triple(tab(0), tab(1), tab(2))).toDF()\n",
        "\n",
        "yago.persist\n",
        "yago.count\n",
        "yago.show(5)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+------------+----------------+--------------------+\n|       sujet|            prop|               objet|\n+------------+----------------+--------------------+\n|    <!PAUS3>|     <hasGender>|              <male>|\n|    <!PAUS3>|<hasMusicalRole>|<wordnet_bass_104...|\n|    <!PAUS3>|   <isCitizenOf>|           <Ukraine>|\n|    <!PAUS3>|     <wasBornIn>|      <Philadelphia>|\n|<!T.O.O.H.!>|     <hasGender>|              <male>|\n+------------+----------------+--------------------+\nonly showing top 5 rows\n\nyago: org.apache.spark.sql.DataFrame = [sujet: string, prop: string ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 3,
          "time" : "Took: 7.443s, at 2017-11-09 12:44"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "7D9A66FAA30645A19E256C4DEEF2EA7B"
      },
      "cell_type" : "markdown",
      "source" : "## Statistiques de base\n\n### 1) Retourner la liste des 10 propriétés les plus fréquentes. La sortie doit être une liste de couples (prop, freq) triée de manière décroissante.\n"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E8E056A00D4945E7A000B824FE72BD6A"
      },
      "cell_type" : "code",
      "source" : [
        "val props = yago.groupBy(\"prop\").count.sort($\"count\".desc).limit(10)\n",
        "\n",
        "props.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+----------------+-------+\n|            prop|  count|\n+----------------+-------+\n|<isAffiliatedTo>|1116512|\n|      <playsFor>| 772092|\n|   <isCitizenOf>| 731207|\n|   <isLocatedIn>| 512925|\n|     <hasGender>| 486528|\n|     <wasBornIn>| 405252|\n|       <actedIn>| 242436|\n|        <diedIn>| 131001|\n|   <hasWonPrize>| 115476|\n| <graduatedFrom>| 112670|\n+----------------+-------+\n\nprops: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [prop: string, count: bigint]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 18,
          "time" : "Took: 0.934s, at 2017-11-07 17:56"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "EE5C93BB363849E99789813498DB7438"
      },
      "cell_type" : "markdown",
      "source" : "### 2) Retourner la liste des 10 noeuds ayant le plus grand degré sortant. \nRappel Le degré sortant d'un noeud n est le nombre de triplets où n est le sujet. \n\nLa sortie doit être une liste de couples (sujet, degré) triée de manière décroissante."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1507470001-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "1EFA0C13C21248B0853A462BBD28ED2A"
      },
      "cell_type" : "code",
      "source" : [
        "val noeudsDegresSortant = yago.groupBy(\"sujet\").count.sort($\"count\".desc).limit(10)\n",
        "noeudsDegresSortant.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+--------------------+-----+\n|               sujet|count|\n+--------------------+-----+\n|       <Ilaiyaraaja>|  878|\n|        <Prem_Nazir>|  471|\n|     <United_States>|  456|\n|   <Deva_(composer)>|  440|\n|       <Adoor_Bhasi>|  383|\n| <M._S._Viswanathan>|  378|\n|         <Mammootty>|  360|\n|    <United_Kingdom>|  345|\n|<Laxmikant–Pyarelal>|  324|\n| <Jagathy_Sreekumar>|  314|\n+--------------------+-----+\n\nnoeudsDegresSortant: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [sujet: string, count: bigint]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 19,
          "time" : "Took: 1.043s, at 2017-11-07 17:56"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "D9A791E89A3E419182F69CAE87E4F03F"
      },
      "cell_type" : "markdown",
      "source" : "### 3) Pour chaque propriété, retourner le nombre de sujets distincts d'où elle démarre ainsi que le nombre d'objets distincts où elle arrive. \n\nLa sortie doit être une liste de tuples (pro, nb-sujets, nb-objets). Attention Un objet (sujet) peut avoir plusieurs fois la même propriété."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "F3C334067D7D48B8922B070ABC14E7B5"
      },
      "cell_type" : "code",
      "source" : [
        "val propSujets = yago.select(\"prop\", \"sujet\").distinct.groupBy(\"prop\").count.withColumnRenamed(\"count\", \"nbSujet\")\n",
        "//propSujets.show(5)\n",
        "\n",
        "val propObjets = yago.select(\"prop\", \"objet\").distinct.groupBy(\"prop\").count.withColumnRenamed(\"count\", \"nbObjet\")\n",
        "//propObjets.show(5)\n",
        "\n",
        "val propStats = propSujets.join(propObjets, \"prop\")\n",
        "propStats.show(10)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+--------------------+-------+-------+\n|                prop|nbSujet|nbObjet|\n+--------------------+-------+-------+\n|<hasOfficialLangu...|   1177|    134|\n|    <isInterestedIn>|    781|    160|\n|         <dealsWith>|    262|    204|\n|       <isLocatedIn>| 114649|  30544|\n|<hasAcademicAdvisor>|   2318|    527|\n|        <isKnownFor>|     68|     56|\n|       <isCitizenOf>| 488398|    451|\n|        <influences>|   4517|   1967|\n|           <exports>|    167|     58|\n|           <actedIn>|  35223|  58160|\n+--------------------+-------+-------+\nonly showing top 10 rows\n\npropSujets: org.apache.spark.sql.DataFrame = [prop: string, nbSujet: bigint]\npropObjets: org.apache.spark.sql.DataFrame = [prop: string, nbObjet: bigint]\npropStats: org.apache.spark.sql.DataFrame = [prop: string, nbSujet: bigint ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 20,
          "time" : "Took: 2.833s, at 2017-11-07 17:56"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "662E245C23AB44E09A347292BF52AC19"
      },
      "cell_type" : "markdown",
      "source" : "### 4) Encoder la fonction noeudDegre(d:entier) qui retourne les noeuds de degrée d. \n\nLe degré d'un noeud = degré sortant + degré entrant."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "6462CDBBA003410C87F17B3DFA776F59"
      },
      "cell_type" : "code",
      "source" : [
        "\n",
        "def noeudDegre(d: Int) : org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]  = {\n",
        "  // Calcul des degrés entrants\n",
        "  val noeudsDegresEntrant = yago.groupBy(\"objet\").count.\n",
        "      where(\"count < \"+d.toString).\n",
        "      withColumnRenamed(\"objet\", \"noeud\").\n",
        "      withColumnRenamed(\"count\", \"degEntrant\")\n",
        "  // Calcul des degrés sortants\n",
        "  val noeudsDegresSortant = yago.groupBy(\"sujet\").count.\n",
        "      where(\"count < \"+d.toString).\n",
        "      withColumnRenamed(\"sujet\", \"noeud\").\n",
        "      withColumnRenamed(\"count\", \"degSortant\")\n",
        "  // Combinaison \n",
        "  val noeudsDegres = noeudsDegresEntrant.join(noeudsDegresSortant, \"noeud\")\n",
        "  noeudsDegres.where(\"degSortant + degEntrant = \"+d.toString)\n",
        "  \n",
        "}\n",
        "\n",
        "\n",
        "noeudDegre(400).show(10)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+-------------------+----------+----------+\n|              noeud|degEntrant|degSortant|\n+-------------------+----------+----------+\n|<North_Carolina_FC>|       395|         5|\n+-------------------+----------+----------+\n\nnoeudDegre: (d: Int)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 21,
          "time" : "Took: 2.143s, at 2017-11-07 17:56"
        }
      ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "C1DDA93C669B49468435D3BAF7AFBCF9"
      },
      "cell_type" : "markdown",
      "source" : "`$ grep \"<North_Carolina_FC>\" yagoFacts5M.txt -c\n400\n`\n\n## Statistiques sur les chemins et co-occurences\n\n### 1) Pour chaque pattern de 2 propriétés qui se suivent, calculer sa fréquence dans les données. \n\nExemple : Si le triple pattern (?x,influences,?y) (?y, livesIn, ?z) retourne 1000 résultats alor la fréquence du pattern (influences, livesIn) vaut 1000."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "D6FF20C564114922846041CD7BFDF95A"
      },
      "cell_type" : "code",
      "source" : [
        "val objets = yago.select(\"objet\",\"prop\").\n",
        "                  withColumnRenamed(\"objet\", \"y\").\n",
        "                  withColumnRenamed(\"prop\",\"prop2\")\n",
        "val sujets = yago.select(\"sujet\", \"prop\").\n",
        "                  withColumnRenamed(\"sujet\",\"y\").\n",
        "                  withColumnRenamed(\"prop\",\"prop1\")\n",
        "\n",
        "val patterns = sujets.join(objets, \"y\").groupBy(\"prop1\", \"prop2\").count()\n",
        "\n",
        "patterns.where(\"count = 5\").show(10)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+--------------------+--------------------+-----+\n|               prop1|               prop2|count|\n+--------------------+--------------------+-----+\n|    <hasMusicalRole>|           <created>|    5|\n|     <wroteMusicFor>|           <created>|    5|\n|            <diedIn>|              <owns>|    5|\n|    <isPoliticianOf>|              <owns>|    5|\n|<hasAcademicAdvisor>|          <hasChild>|    5|\n|    <isAffiliatedTo>|<hasAcademicAdvisor>|    5|\n|        <hasCapital>|       <hasCurrency>|    5|\n+--------------------+--------------------+-----+\n\nobjets: org.apache.spark.sql.DataFrame = [y: string, prop2: string]\nsujets: org.apache.spark.sql.DataFrame = [y: string, prop1: string]\npatterns: org.apache.spark.sql.DataFrame = [prop1: string, prop2: string ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 22,
          "time" : "Took: 11.812s, at 2017-11-07 17:57"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "8A7D8909780D4AA080ECA82990485932"
      },
      "cell_type" : "markdown",
      "source" : "### 2) Encoder la fonction cheminNoeudLongueur(noeud: string, len:entier) qui retourne, pour le sujet noeud, tous les chemins démarrant de noeud et ayant la longueur len. \n\nLa longueur d'un chemin est le nombre de propriétés traversées."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "BA6BB58A98284C60AF74C4F91E71E732"
      },
      "cell_type" : "code",
      "source" : [
        "\n",
        "def cheminNoeudLongueur(noeud: String, len: Int) : Array[org.apache.spark.sql.Row]  = {\n",
        "  /* Si la longueur voulue est 1, on peut renvoyer directement la requête SQL */\n",
        "    if (len <= 1) yago.where(\"sujet like \\\"\"+noeud+\"\\\"\").collect()\n",
        "  /* Sinon si la longueur [len] est plus grande, on cherche tous les chemins de longueur [len] - 1, \n",
        "    auxquels on ajoute tous les chemins de longueur 1 qui partent de ces chemins.*/\n",
        "    else cheminNoeudLongueur(noeud, len-1).\n",
        "          flatMap(row => yago.where(\"sujet like \\\"\"+row.getString(row.length-1)+\"\\\"\").collect().\n",
        "          map(end => Row.fromSeq(row.toSeq.slice(0,row.length-1) ++ end.toSeq)))\n",
        "}\n",
        "println(\"Chemins de longueur 1\")\n",
        "cheminNoeudLongueur(\"<1-UP_Studio>\", 1).take(25).foreach(println)\n",
        "println(\"Chemins de longueur 2\")\n",
        "cheminNoeudLongueur(\"<1-UP_Studio>\", 2).take(25).foreach(println)\n",
        "println(\"Chemins de longueur 3\")\n",
        "cheminNoeudLongueur(\"<1-UP_Studio>\", 3).take(25).foreach(println)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "Chemins de longueur 1\n[<1-UP_Studio>,<created>,<Blue_Dragon_Plus>]\n[<1-UP_Studio>,<created>,<Mother_(series)>]\n[<1-UP_Studio>,<created>,<Super_Mario_3D_World>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>]\n[<1-UP_Studio>,<isLocatedIn>,<de/Tokio>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>]\nChemins de longueur 2\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Kantō_region>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Honshu>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Asia>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Kantō_region>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Tokyo>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Honshu>]\nChemins de longueur 3\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Hong_Kong>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Australia>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<United_States>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Thailand>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<China>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<South_Korea>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<hasCapital>,<Tokyo>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<hasCurrency>,<Japanese_yen>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<hasOfficialLanguage>,<Japanese_language>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<isLocatedIn>,<Asia>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Korean_conflict>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Battle_of_Guilin–Liuzhou>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Operation_Ocean_Shield>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Mongol_invasions_of_Japan>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<First_Indochina_War>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Operation_Enduring_Freedom_–_Horn_of_Africa>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Operation_Enduring_Freedom>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Korean_War>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Moro_conflict>]\n[<1-UP_Studio>,<isLocatedIn>,<Tokyo>,<isLocatedIn>,<Japan>,<participatedIn>,<Vietnam_War>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Hong_Kong>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Australia>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<United_States>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<Thailand>]\n[<1-UP_Studio>,<isLocatedIn>,<Chiyoda,_Tokyo>,<isLocatedIn>,<Japan>,<dealsWith>,<China>]\ncheminNoeudLongueur: (noeud: String, len: Int)Array[org.apache.spark.sql.Row]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 23,
          "time" : "Took: 5.245s, at 2017-11-07 17:57"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "0F4673737CA5437B8AE748E31DD9AE31"
      },
      "cell_type" : "markdown",
      "source" : "### 3) Pour chaque paire de propriétés, donner le nombre de sujets qu'elles partagent. \n\nExemple. Si le triple pattern (x, livesIn, y) (x, citizenOf, z) retourne 10 résultat alors les propriétés de la paire (livesIn, citizenOf) partagent 10 sujets."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "0326122B93F84A4A8DE3EBDA0C65F2F9"
      },
      "cell_type" : "code",
      "source" : [
        "val prop1 = yago.withColumnRenamed(\"sujet\",\"x\").withColumnRenamed(\"prop\",\"prop1\")\n",
        "val prop2 = yago.withColumnRenamed(\"sujet\",\"x\").withColumnRenamed(\"prop\",\"prop2\")\n",
        "\n",
        "val paireFreq = prop1.join(prop2,\"x\").\n",
        "      where(\"prop1 < prop2\").\n",
        "      groupBy(\"prop1\",\"prop2\").\n",
        "      count().\n",
        "      withColumnRenamed(\"count\", \"sujetsPartages\")\n",
        "\n",
        "paireFreq.orderBy($\"sujetsPartages\".desc).show(10)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+----------------+----------------+--------------+\n|           prop1|           prop2|sujetsPartages|\n+----------------+----------------+--------------+\n|<isAffiliatedTo>|      <playsFor>|       6017893|\n|<isAffiliatedTo>|   <isCitizenOf>|       1655218|\n|   <isCitizenOf>|      <playsFor>|       1174284|\n|     <hasGender>|<isAffiliatedTo>|       1049416|\n|<isAffiliatedTo>|     <wasBornIn>|       1003512|\n|     <hasGender>|      <playsFor>|        718536|\n|     <hasGender>|   <isCitizenOf>|        714173|\n|      <playsFor>|     <wasBornIn>|        685441|\n|   <isCitizenOf>|     <wasBornIn>|        594215|\n|       <created>| <wroteMusicFor>|        521147|\n+----------------+----------------+--------------+\nonly showing top 10 rows\n\nprop1: org.apache.spark.sql.DataFrame = [x: string, prop1: string ... 1 more field]\nprop2: org.apache.spark.sql.DataFrame = [x: string, prop2: string ... 1 more field]\npaireFreq: org.apache.spark.sql.DataFrame = [prop1: string, prop2: string ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 24,
          "time" : "Took: 4.213s, at 2017-11-07 17:57"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "9F20FA6D596748D18580C9E5BF0F1194"
      },
      "cell_type" : "markdown",
      "source" : "## Bonus\n\n** Copie du dataset : **\n\n````tar xzvf /Infos/bd/spark/dataset/dbpedia/dbpediaShortName8M.tgz\ncp /Infos/bd/spark/dataset/dbpedia/dbpediaShortNameTypeFor8M.txt /tmp/BDLE_3774877/dataset ````\n\n### 1) Dans un premier temps, compléter les triplets de dbpediaShortName8M avec le type de leurs noeuds qui se trouvent dans dbpediaShortNameTypeFor8M.txt."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "E8790A27B9EB4708B23BDBC7DB87C29B"
      },
      "cell_type" : "code",
      "source" : [
        "val shortName = dataset + \"dbpediaShortName8M.txt\"\n",
        "val shortNameType = dataset + \"dbpediaShortNameTypeFor8M.txt\"\n",
        "\n",
        "val dbPedia = sc.textFile(shortName).\n",
        "  map(ligne => ligne.split(\"\\\\t\")).coalesce(8).\n",
        "  map(tab => Triple(tab(0), tab(1), tab(2))).toDS()\n",
        "\n",
        "println(\"dbPedia : \"+dbPedia.count +\" éléments\")\n",
        "dbPedia.persist\n",
        "dbPedia.show(5)\n",
        "\n",
        "val dbPediaType = sc.textFile(shortNameType).\n",
        "  map(ligne => ligne.split(\"\\\\t\")).coalesce(8).\n",
        "  map(tab => Triple(tab(0), tab(1), tab(2))).toDS()\n",
        "\n",
        "println(\"dbPediaType : \"+dbPediaType.count +\" éléments\")\n",
        "dbPediaType.persist\n",
        "dbPediaType.show(5)\n",
        "\n",
        "val dbPediaAll = dbPedia.union(dbPediaType)\n",
        "\n",
        "println(\"dbPediaAll : \"+dbPediaAll.count +\" éléments\")\n",
        "dbPediaAll.persist\n",
        "dbPediaAll.show(5)\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "dbPedia : 8288566 éléments\n+-----+----------------+--------------------+\n|sujet|            prop|               objet|\n+-----+----------------+--------------------+\n|<!!!>|<associatedActs>|<Turing_Machine_(...|\n|<!!!>|<associatedActs>|           <Out_Hud>|\n|<!!!>|<associatedActs>|   <LCD_Soundsystem>|\n|<!!!>|<associatedActs>|   <Maserati_(band)>|\n|<!!!>|<associatedActs>|  <The_Juan_MacLean>|\n+-----+----------------+--------------------+\nonly showing top 5 rows\n\ndbPediaType : 108145 éléments\n+--------------------+------+--------------------+\n|               sujet|  prop|               objet|\n+--------------------+------+--------------------+\n|     <!Hero_(album)>|<type>|      <Double_album>|\n|          <$ell.Out>|<type>|             <Album>|\n|<%22Ibrahim_Kodra...|<type>|<Non-governmental...|\n|<%22No_Snow,_No_S...|<type>|        <Live_album>|\n|<%22Ridgeriders%2...|<type>|        <Live_album>|\n+--------------------+------+--------------------+\nonly showing top 5 rows\n\ndbPediaAll : 8396711 éléments\n+-----+----------------+--------------------+\n|sujet|            prop|               objet|\n+-----+----------------+--------------------+\n|<!!!>|<associatedActs>|<Turing_Machine_(...|\n|<!!!>|<associatedActs>|           <Out_Hud>|\n|<!!!>|<associatedActs>|   <LCD_Soundsystem>|\n|<!!!>|<associatedActs>|   <Maserati_(band)>|\n|<!!!>|<associatedActs>|  <The_Juan_MacLean>|\n+-----+----------------+--------------------+\nonly showing top 5 rows\n\nshortName: String = /tmp/BDLE_3774877/dataset/dbpediaShortName8M.txt\nshortNameType: String = /tmp/BDLE_3774877/dataset/dbpediaShortNameTypeFor8M.txt\ndbPedia: org.apache.spark.sql.Dataset[Triple] = [sujet: string, prop: string ... 1 more field]\ndbPediaType: org.apache.spark.sql.Dataset[Triple] = [sujet: string, prop: string ... 1 more field]\ndbPediaAll: org.apache.spark.sql.Dataset[Triple] = [sujet: string, prop: string ... 1 more field]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 2,
          "time" : "Took: 13.763s, at 2017-11-09 13:12"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "D56293D004F044F686A4D15D1C974770"
      },
      "cell_type" : "markdown",
      "source" : "### 2) Pour chaque type, retourner son domaine, i.e le nombre de sujets distincts ayant ce type."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "EAC4CD51C65A40DE922CBD37C99E22AD"
      },
      "cell_type" : "code",
      "source" : [
        "val domaines = dbPediaAll.select(\"sujet\",\"objet\").\n",
        "    where(\"prop = '<type>'\").\n",
        "    groupBy(\"objet\").\n",
        "    count.\n",
        "    withColumnRenamed(\"objet\", \"type\").\n",
        "    withColumnRenamed(\"count\", \"domaine\")\n",
        "\n",
        "domaines.orderBy($\"domaine\".desc).show(10)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+--------------------+-------+\n|                type|domaine|\n+--------------------+-------+\n|             <Album>|  26044|\n|      <State_school>|   6207|\n| <Public_university>|   3907|\n|    <Private_school>|   3674|\n|<Privately_held_c...|   3127|\n|<Private_university>|   2314|\n|        <Live_album>|   2276|\n|<Mixed-sex_educat...|   1991|\n|    <Public_company>|   1978|\n| <Compilation_album>|   1841|\n+--------------------+-------+\nonly showing top 10 rows\n\ndomaines: org.apache.spark.sql.DataFrame = [type: string, domaine: bigint]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 5,
          "time" : "Took: 1.552s, at 2017-11-09 13:14"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "50E4B737FA5B4195804E8D80D3EAEB3F"
      },
      "cell_type" : "markdown",
      "source" : "### 3) Pour chaque type, retourner son co-domaine, i.e le nombre d'objets distincts ayant ce type.\n\nIl faut trouver tous les objets distincts dans la table ainsi que leurs types."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "3558E32E3F9445BB9C6D08851AD73FF8"
      },
      "cell_type" : "code",
      "source" : [
        "// Tous les sujets et leur type\n",
        "val types = dbPediaAll.select(\"sujet\",\"objet\").\n",
        "      where(\"prop = '<type>'\").\n",
        "      withColumnRenamed(\"objet\",\"type\").\n",
        "      withColumnRenamed(\"sujet\",\"objet\")\n",
        "\n",
        "// Tous les objets qui ne sont pas des types\n",
        "val objets = dbPediaAll.select(\"objet\").where(\"prop != '<type>'\")\n",
        "\n",
        "\n",
        "val res = types.join(objets,\"objet\").distinct.groupBy(\"type\").count\n",
        "\n",
        "res.orderBy($\"count\".desc).show(10)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "+--------------------+-----+\n|                type|count|\n+--------------------+-----+\n| <Public_university>| 1611|\n|<Private_university>|  745|\n|    <Private_school>|  727|\n|             <Album>|  677|\n|      <State_school>|  406|\n|    <Public_company>|  347|\n|<Privately_held_c...|  345|\n|<Promotional_reco...|  264|\n|<Mixed-sex_educat...|  200|\n|    <Beauty_pageant>|  159|\n+--------------------+-----+\nonly showing top 10 rows\n\ntypes: org.apache.spark.sql.DataFrame = [objet: string, type: string]\nobjets: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [objet: string]\nres: org.apache.spark.sql.DataFrame = [type: string, count: bigint]\n"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 7,
          "time" : "Took: 4.451s, at 2017-11-09 13:14"
        }
      ]
    },
    {
      "metadata" : {
        "id" : "CAE23385BCDB498285EE0EB87712FCB2"
      },
      "cell_type" : "markdown",
      "source" : "On peut vérifier pour le type `<Private_school>` par exemple :\n\nOn récupère tous les sujets du type voulu, on stocke dans un fichier :\n\n> `grep \".*\\s*<type>\\s*<Private_school>\" dbpediaShortNameTypeFor8M.txt | cut  -f1 > priv.txt`\n\n> `grep -c \".*\" priv.txt `\n\n> 3674\n\n\nOn cherche ces sujets en tant qu'objet dans le fichier dbPedia original, on stock le résultat dans un fichier :\n\n> `while read line; do grep \"<.*>\\s*<.*>\\s*$line\" dbpediaShortName8M.txt ; done < priv.txt > priv2.txt`\n\nOn récupère le nombre d'objets distincts :\n\n> `cut priv2.txt -f3 | uniq -c | grep -c \".*\"` \n\n> 727"
    }
  ],
  "nbformat" : 4
}